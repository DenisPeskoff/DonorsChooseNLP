{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "from torchtext.data.field import Field\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set GPU flag to false if running on CPU\n",
    "GPU = True\n",
    "batch_size = 32\n",
    "column1 = 'project_essay_1'\n",
    "column2 = 'project_essay_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(include_lengths=True, batch_first = True)\n",
    "ID = data.Field(sequential=False)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "\n",
    "def sort_key(ex):\n",
    "    return len(getattr(ex,column1))\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='.', train='train.csv',\n",
    "        validation='val.csv', test='dev.csv', format='csv', skip_header=True,\n",
    "        fields=[('id', ID), ('project_title', TEXT),('project_resource_summary', TEXT), \n",
    "                ('project_essay_1', TEXT), ('project_essay_2', TEXT), ('project_is_approved', LABEL)])\n",
    "\n",
    "#vocab is shared across all the text fields\n",
    "#CAUTION: GloVe will download all embeddings locally (862 MB).  If not interested, remove \"vectors\"\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
    "ID.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "#change device to 0 for GPU\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train, val, test), repeat=False, sort=False,#sort_key= sort_key, sort_within_batch=True,\n",
    "        batch_size=(batch_size), device=-1)#-1 if GPU else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Dict\n",
    "class DAN(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        #initialize the vocab from the text field (passed in from train_iter) and pad\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "\n",
    "        \n",
    "        #run the vocab through Glove Embeddings\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "        \n",
    "        #set the unknown items to the mean embedding and make them cuda()\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        #self.text_embeddings.weight.requires_grad = False \n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        #the classifier converts the hidden dimensions into the answers.  \n",
    "        #It takes batch norm and dropout as well.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "\n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        #if the text exists, run it through embeddings, pool, dropout, and then run it through a hidden layer\n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            #print(embed.sum(0))\n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnormed_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnormed_dropped)\n",
    "            return self.classifier(nonlinear )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def validate(net,iter_obj, field):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    net.zero_grad()\n",
    "    iter_obj.init_epoch()\n",
    "    pred = []\n",
    "    actual = []\n",
    "    dev_loss = []\n",
    "    for batch in iter_obj:\n",
    "        #input1,input2,label = getattr(batch,column1)[0], getattr(batch,column2)[0], batch.project_is_approved-1\n",
    "        #if(GPU): input1, label, input2 = input1.cuda(), label.cuda(), input2.cuda()\n",
    "        text, lengths = getattr(batch,field)\n",
    "        text = text.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "        input_dict = {}\n",
    "        lengths_dict = {}\n",
    "        input_dict['text'] = text\n",
    "        lengths_dict['text'] = lengths\n",
    "            \n",
    "        scores = net(input_dict,lengths_dict)\n",
    "        dev_loss.append(float(loss_function(scores,label)))\n",
    "        pred.extend(scores.cpu().data.numpy().argmax(axis=1))\n",
    "        actual.extend(label.data.cpu().numpy().tolist())\n",
    "    return  round(np.mean(dev_loss),4), round(accuracy_score(actual,pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields: Dict[str, Field] = train_iter.dataset.fields\n",
    "ANSWER_SIZE = 2\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 1000 \n",
    "\n",
    "def train_net(NUM_EPOCHS, field):\n",
    "    model =  DAN(EMBEDDING_DIM, HIDDEN_DIM, fields[field], ANSWER_SIZE)\n",
    "    #LSTMClassifier(vocab = TEXT.vocab, embedding_dim=300, vocab_size=len(TEXT.vocab), \n",
    "    #                       hidden_dim=50, batch_size=batch_size, label_size=2)\n",
    "     \n",
    "    model = model.cuda()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "    n_correct = 0.\n",
    "    n_total = 0.\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        train_loss = []\n",
    "        train_iter.init_epoch()\n",
    "        for batch in train_iter:\n",
    "            input_dict = {}\n",
    "            lengths_dict = {}\n",
    "            if hasattr(batch, 'project_essay_1'):\n",
    "                text, lengths = getattr(batch,field)\n",
    "                text = text.cuda()\n",
    "                lengths = lengths.cuda()\n",
    "                input_dict['text'] = text\n",
    "                lengths_dict['text'] = lengths\n",
    "            \n",
    "            model.train()\n",
    "            model.zero_grad()       \n",
    "            #input1,input2,label = getattr(batch,column1)[0], getattr(batch,column2)[0], batch.project_is_approved-1\n",
    "            #if(GPU): input1, label, input2 = input1.cuda(), label.cuda(), input2.cuda()\n",
    "            \n",
    "            scores = model(input_dict, lengths_dict)\n",
    "            #scores = model(input1,input2)\n",
    "            loss = loss_function(scores, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            n_correct += int((torch.max(scores, 1)[1].view(label.size()) == label).sum())\n",
    "            n_total += batch.batch_size\n",
    "            train_acc = 100. * n_correct/n_total\n",
    "            train_loss.append(loss.data[0])\n",
    "\n",
    "        if(NUM_EPOCHS<=10) or ((i+1)%10==0):\n",
    "            val_loss, val_acc = validate(model,val_iter, field)\n",
    "            print(f\"Epoch {i+1} Train loss: {round(np.mean(train_loss),4)} Val loss: {val_loss} Train acc: {train_acc} Val acc: {val_acc}\")\n",
    "    print (\"TEST ACC:\", validate(model, test_iter, field))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cliphomes/dpeskov/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:21: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "/cliphomes/dpeskov/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train loss: 0.6925 Val loss: 0.6544 Train acc: 58.29333333333334 Val acc: 0.6181\n",
      "Epoch 2 Train loss: 0.6496 Val loss: 0.6542 Train acc: 60.435 Val acc: 0.6263\n",
      "Epoch 3 Train loss: 0.6139 Val loss: 0.6628 Train acc: 62.681111111111115 Val acc: 0.6194\n",
      "Epoch 4 Train loss: 0.5822 Val loss: 0.6889 Train acc: 64.605 Val acc: 0.601\n",
      "Epoch 5 Train loss: 0.5513 Val loss: 0.7063 Train acc: 66.148 Val acc: 0.6002\n",
      "TEST ACC: (0.7027, 0.6059)\n",
      "Epoch 1 Train loss: 0.7053 Val loss: 0.6811 Train acc: 54.233333333333334 Val acc: 0.5702\n",
      "Epoch 2 Train loss: 0.671 Val loss: 0.6837 Train acc: 56.565 Val acc: 0.5701\n",
      "Epoch 3 Train loss: 0.6393 Val loss: 0.6938 Train acc: 58.992222222222225 Val acc: 0.5758\n",
      "Epoch 4 Train loss: 0.6041 Val loss: 0.728 Train acc: 61.094166666666666 Val acc: 0.5675\n",
      "Epoch 5 Train loss: 0.576 Val loss: 0.7344 Train acc: 62.962666666666664 Val acc: 0.5601\n",
      "TEST ACC: (0.7281, 0.5626)\n",
      "CPU times: user 1min 38s, sys: 46.7 s, total: 2min 25s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Combining essay 1 and essay 2\n",
    "#model = train_net(5, 'project_essay_1')\n",
    "#model = train_net(5, 'project_essay_2')\n",
    "model = train_net(5, 'project_resource_summary')\n",
    "model = train_net(5, 'project_title')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"TEST ACC:\", validate(model, test_iter, 'project_essay_1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## Combining essay 1 and essay 2\n",
    "model = train_net(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
