{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Set GPU flag to false if running on CPU\n",
    "GPU = True\n",
    "\n",
    "# Change the column to use either essay1/2 or title \n",
    "column = 'project_essay_1'\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT = data.Field(include_lengths=True,batch_first=True)\n",
    "ID = data.Field(sequential=False)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "\n",
    "def sort_key(ex):\n",
    "    return len(getattr(ex,column))\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='.', train='train.csv',\n",
    "        validation='val.csv', test='dev.csv', format='csv', skip_header=True,\n",
    "        fields=[('id', ID), ('project_title', None),('project_resource_summary', None), \n",
    "                ('project_essay_1', TEXT), ('project_essay_2', None), ('project_is_approved', LABEL)])\n",
    "\n",
    "#vocab is shared across all the text fields\n",
    "#CAUTION: GloVe will download all embeddings locally (862 MB).  If not interested, remove \"vectors\"\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
    "ID.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "#change device to 0 for GPU\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train, val, test), sort_key= sort_key, repeat=False, sort_within_batch=True,\n",
    "        batch_size=(batch_size), device=-1 if GPU else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "- Naive LSTM/BiLSTM classifier with packed sequences for variable length inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embedding_dim, hidden_dim, vocab_size, label_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(vocab.vectors)        \n",
    "        self.lstm = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim, label_size)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        if(GPU):\n",
    "            h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda())\n",
    "#             c0 = Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda())\n",
    "        \n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(2, batch_size, self.hidden_dim))\n",
    "            c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim))\n",
    "            \n",
    "        return h0\n",
    "#         return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence, lengths):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        packed_emb = nn.utils.rnn.pack_padded_sequence(embeds, list(lengths.data), batch_first=True)\n",
    "        lstm_out, self.hidden = self.lstm(packed_emb, self.hidden)\n",
    "        y = self.hidden2label(self.hidden[0].squeeze(0))\n",
    "        return y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def validate(net,iter_obj):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    iter_obj.init_epoch()\n",
    "    pred = []\n",
    "    actual = []\n",
    "    dev_loss = []\n",
    "    for batch in iter_obj:\n",
    "        input,label,length = getattr(batch,column)[0], batch.project_is_approved-1, Variable(getattr(batch,column)[1])\n",
    "        if(GPU): input, label, length = input.cuda(), label.cuda(),length.cuda()\n",
    "        net.hidden = net.init_hidden(input.shape[0])\n",
    "        scores = net(input,length)\n",
    "        dev_loss.append(loss_function(scores,label).data[0])\n",
    "        pred.extend(scores.cpu().data.numpy().argmax(axis=1))\n",
    "        actual.extend(label.data.cpu().numpy().tolist())\n",
    "    return  round(np.mean(dev_loss),4), round(accuracy_score(actual,pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(NUM_EPOCHS):\n",
    "    model = LSTMClassifier(vocab = TEXT.vocab, embedding_dim=300, vocab_size=len(TEXT.vocab), \n",
    "                           hidden_dim=50, batch_size=batch_size, label_size=2)\n",
    "    \n",
    "    if(GPU): model.cuda()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)#, momentum=0.9)\n",
    "\n",
    "    n_correct = 0.\n",
    "    n_total = 0.\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        train_loss = []\n",
    "        train_iter.init_epoch()\n",
    "        for batch in train_iter:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()       \n",
    "            input,label,length = getattr(batch,column)[0], batch.project_is_approved-1, Variable(getattr(batch,column)[1])\n",
    "            if(GPU): input,label,length = input.cuda(), label.cuda(), length.cuda()\n",
    "            model.hidden = model.init_hidden(input.shape[0])\n",
    "\n",
    "            scores = model(input,length)\n",
    "            loss = loss_function(scores, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_correct += (torch.max(scores, 1)[1].view(label.size()) == label).sum().data[0]\n",
    "            n_total += batch.batch_size\n",
    "            train_acc = 100. * n_correct/n_total\n",
    "            train_loss.append(loss.data[0])\n",
    "\n",
    "        if(NUM_EPOCHS<=10) or ((i+1)%10==0):\n",
    "            val_loss, val_acc = validate(model,val_iter)\n",
    "            print(f\"Epoch {i+1} Train loss: {round(np.mean(train_loss),4)} Val loss: {val_loss} Train acc: {train_acc} Val acc: {val_acc}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train loss: 0.6944 Val loss: 0.6943 Train acc: 50.31666666666667 Val acc: 0.5023\n",
      "Epoch 2 Train loss: 0.6685 Val loss: 0.7102 Train acc: 54.655 Val acc: 0.5119\n",
      "Epoch 3 Train loss: 0.5406 Val loss: 0.8053 Train acc: 60.58555555555556 Val acc: 0.5241\n",
      "Epoch 4 Train loss: 0.3411 Val loss: 1.0044 Train acc: 66.69916666666667 Val acc: 0.5186\n",
      "Epoch 5 Train loss: 0.2073 Val loss: 1.2339 Train acc: 71.738 Val acc: 0.522\n",
      "Epoch 6 Train loss: 0.1457 Val loss: 1.3914 Train acc: 75.48833333333333 Val acc: 0.5183\n",
      "Epoch 7 Train loss: 0.1139 Val loss: 1.5017 Train acc: 78.2995238095238 Val acc: 0.5192\n",
      "Epoch 8 Train loss: 0.0929 Val loss: 1.6781 Train acc: 80.44958333333334 Val acc: 0.5172\n",
      "Epoch 9 Train loss: 0.0773 Val loss: 2.0785 Train acc: 82.15333333333334 Val acc: 0.5155\n",
      "Epoch 10 Train loss: 0.0673 Val loss: 2.0625 Train acc: 83.53966666666666 Val acc: 0.5173\n"
     ]
    }
   ],
   "source": [
    "# Batch size 32 optimizer Adam(0.001) epochs 10\n",
    "model = train_net(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train loss: 0.0637 Val loss: 2.2588 Train acc: 83.633 Val acc: 0.515\n",
      "Epoch 20 Train loss: 0.0478 Val loss: 2.5291 Train acc: 90.09083333333334 Val acc: 0.5138\n",
      "Epoch 30 Train loss: 0.0434 Val loss: 2.6984 Train acc: 92.37011111111111 Val acc: 0.5112\n",
      "Epoch 40 Train loss: 0.0425 Val loss: 2.3447 Train acc: 93.54775 Val acc: 0.5207\n",
      "Epoch 50 Train loss: 0.0421 Val loss: 2.3905 Train acc: 94.28046666666667 Val acc: 0.5162\n",
      "Epoch 60 Train loss: 0.0404 Val loss: 2.4795 Train acc: 94.77722222222222 Val acc: 0.5178\n",
      "Epoch 70 Train loss: 0.0392 Val loss: 2.572 Train acc: 95.1437619047619 Val acc: 0.5173\n",
      "Epoch 80 Train loss: 0.0413 Val loss: 2.4635 Train acc: 95.414375 Val acc: 0.5094\n",
      "Epoch 90 Train loss: 0.0371 Val loss: 2.7861 Train acc: 95.63466666666666 Val acc: 0.5177\n",
      "Epoch 100 Train loss: 0.04 Val loss: 2.3652 Train acc: 95.8118 Val acc: 0.5102\n"
     ]
    }
   ],
   "source": [
    "# Batch size 32 optimizer Adam(0.001) epochs 100\n",
    "model = train_net(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
