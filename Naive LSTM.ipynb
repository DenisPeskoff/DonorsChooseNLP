{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Set GPU flag to false if running on CPU\n",
    "GPU = True\n",
    "\n",
    "# Change the column to use either essay1/2, title or resource summary\n",
    "column = 'project_essay_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT = data.Field(include_lengths=True)\n",
    "ID = data.Field(sequential=False)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "def sort_key(ex):\n",
    "    return len(vars(ex)[column])\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='.', train='train.csv',\n",
    "        validation='val.csv', test='dev.csv', format='csv', skip_header=True,\n",
    "        fields=[('id', ID), ('project_title', TEXT),('project_resource_summary', None), \n",
    "                ('project_essay_1', TEXT), ('project_essay_2', TEXT), ('project_is_approved', LABEL)])\n",
    "\n",
    "#vocab is shared across all the text fields\n",
    "#CAUTION: GloVe will download all embeddings locally (862 MB).  If not interested, remove \"vectors\"\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
    "ID.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "#change device to 0 for GPU\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, val, test), sort_key= sort_key, repeat=False,\n",
    "        batch_size=(64), device=-1 if GPU else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive LSTM/BiLSTM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embedding_dim, hidden_dim, vocab_size, label_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(vocab.vectors)        \n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(2*hidden_dim, label_size)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        if(GPU):\n",
    "            h0 = Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda())\n",
    "            c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda())\n",
    "        \n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(2, batch_size, self.hidden_dim))\n",
    "            c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim))\n",
    "            \n",
    "        return (h0, c0)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        y  = self.hidden2label(lstm_out[-1])\n",
    "        return y\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def test_result(net,iter_obj):\n",
    "    pred = []\n",
    "    actual = []\n",
    "    total_loss = 0.\n",
    "    total = 0.\n",
    "    for batch in iter_obj:\n",
    "        input,label = vars(batch)[column][0], batch.project_is_approved-1\n",
    "        if(GPU): input, label = input.cuda(), label.cuda()\n",
    "        net.hidden = net.init_hidden(input.shape[1])\n",
    "        scores = net(input)\n",
    "        loss = loss_function(scores,label)\n",
    "        total_loss += loss.data[0]\n",
    "        total += len(batch)\n",
    "        pred.extend(scores.cpu().data.numpy().argmax(axis=1))\n",
    "        actual.extend(label.data.cpu().numpy().tolist())\n",
    "    return  round(total_loss/total,4), round(accuracy_score(actual,pred),4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Naive BiLSTM model\n",
    "- Input: project_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train loss: 0.010834628168741861 Val loss: 0.010856181871891022 Train acc: 0.5004 Val acc: 0.4982\n",
      "Epoch 2 Train loss: 0.010817983076969783 Val loss: 0.010942785966396331 Train acc: 0.5061 Val acc: 0.5313\n",
      "Epoch 3 Train loss: 0.010731731692949931 Val loss: 0.011484366422891617 Train acc: 0.5114 Val acc: 0.5037\n",
      "Epoch 4 Train loss: 0.010595243789752325 Val loss: 0.014691692617535591 Train acc: 0.5171 Val acc: 0.5027\n",
      "Epoch 5 Train loss: 0.010465758776664735 Val loss: 0.016222913306951522 Train acc: 0.5227 Val acc: 0.5048\n",
      "Epoch 6 Train loss: 0.010424254333972932 Val loss: 0.016707074457406997 Train acc: 0.5233 Val acc: 0.5006\n",
      "Epoch 7 Train loss: 0.010350942740837733 Val loss: 0.016917417538166047 Train acc: 0.5257 Val acc: 0.4989\n",
      "Epoch 8 Train loss: 0.010320433169603347 Val loss: 0.016894089859724046 Train acc: 0.5279 Val acc: 0.5016\n",
      "Epoch 9 Train loss: 0.010240207606554032 Val loss: 0.019885657703876496 Train acc: 0.5298 Val acc: 0.5142\n",
      "Epoch 10 Train loss: 0.010229211755593617 Val loss: 0.02305570331811905 Train acc: 0.5331 Val acc: 0.5083\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import time\n",
    "NUM_EPOCHS = 10\n",
    "model = LSTMClassifier(vocab = TEXT.vocab, embedding_dim=300, vocab_size=len(TEXT.vocab), \n",
    "                       hidden_dim=50, batch_size=64, label_size=2)\n",
    "\n",
    "\n",
    "\n",
    "if(GPU): model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_l = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    start = time.time()\n",
    "    for batch in train_iter:\n",
    "        model.zero_grad()       \n",
    "        input,label = vars(batch)[column][0], batch.project_is_approved-1\n",
    "        if(GPU): input,label = input.cuda(),label.cuda()\n",
    "        model.hidden = model.init_hidden(input.shape[1])\n",
    "\n",
    "        \n",
    "        scores = model(input)\n",
    "        loss = loss_function(scores, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_l.append(loss.cpu().data.numpy())\n",
    "    train_loss, train_acc = test_result(model,train_iter)\n",
    "    val_loss, val_acc = test_result(model,val_iter)\n",
    "    print(f\"Epoch {i+1} Train loss: {train_loss} Val loss: {val_loss} Train acc: {train_acc} Val acc: {val_acc}\")\n",
    "#     if((i+1)%10==0):\n",
    "#     print(f\"End of {i+1} epoch(s)\")\n",
    "#     print(f\"Train accuracy: {test_result(model,train_iter)}, Validation accuracy: {test_result(model,val_iter)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End of 10 epoch(s)\n",
      "Train loss: 0.009447158187627792 Val loss: 0.021745308941602707 Train acc: 0.5742 Val acc: 0.5219\n",
      "End of 20 epoch(s)\n",
      "Train loss: 0.005528450036048889 Val loss: 0.028323950338363647 Train acc: 0.778 Val acc: 0.5269\n",
      "End of 30 epoch(s)\n",
      "Train loss: 0.004875019164383412 Val loss: 0.029892107379436494 Train acc: 0.7932 Val acc: 0.5284\n",
      "End of 40 epoch(s)\n",
      "Train loss: 0.004605301079154015 Val loss: 0.03133709924221039 Train acc: 0.8007 Val acc: 0.5312\n",
      "End of 50 epoch(s)\n",
      "Train loss: 0.004518949463963509 Val loss: 0.030084283912181854 Train acc: 0.8026 Val acc: 0.5284\n",
      "End of 60 epoch(s)\n",
      "Train loss: 0.0044067384536067645 Val loss: 0.03323603180646896 Train acc: 0.8041 Val acc: 0.5308\n",
      "End of 70 epoch(s)\n",
      "Train loss: 0.004350695995986462 Val loss: 0.031539781284332276 Train acc: 0.8052 Val acc: 0.5288\n",
      "End of 80 epoch(s)\n",
      "Train loss: 0.004292706365386645 Val loss: 0.03157402411699295 Train acc: 0.8049 Val acc: 0.5318\n",
      "End of 90 epoch(s)\n",
      "Train loss: 0.0042093737542629244 Val loss: 0.031890649688243865 Train acc: 0.8086 Val acc: 0.5287\n",
      "End of 100 epoch(s)\n",
      "Train loss: 0.004178363389273484 Val loss: 0.03433652529716492 Train acc: 0.8104 Val acc: 0.529\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "import time\n",
    "NUM_EPOCHS = 100\n",
    "model = LSTMClassifier(vocab = TEXT.vocab, embedding_dim=300, vocab_size=len(TEXT.vocab), \n",
    "                       hidden_dim=50, batch_size=64, label_size=2)\n",
    "\n",
    "\n",
    "\n",
    "if(GPU): model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "loss_l = []\n",
    "for i in range(NUM_EPOCHS):\n",
    "    start = time.time()\n",
    "    for batch in train_iter:\n",
    "        model.zero_grad()       \n",
    "        input,label = vars(batch)[column][0], batch.project_is_approved-1\n",
    "        if(GPU): input,label = input.cuda(),label.cuda()\n",
    "        model.hidden = model.init_hidden(input.shape[1])\n",
    "\n",
    "        \n",
    "        scores = model(input)\n",
    "        loss = loss_function(scores, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    loss_l.append(loss.cpu().data.numpy())\n",
    "#     train_loss, train_acc = test_result(model,train_iter)\n",
    "#     val_loss, val_acc = test_result(model,val_iter)\n",
    "#     print(f\"Epoch {i+1} Train loss: {train_loss} Val loss: {val_loss} Train acc: {train_acc} Val acc: {val_acc}\")\n",
    "    if((i+1)%10==0):\n",
    "        train_loss, train_acc = test_result(model,train_iter)\n",
    "        val_loss, val_acc = test_result(model,val_iter)\n",
    "\n",
    "        print(f\"End of {i+1} epoch(s)\")\n",
    "        print(f\"Train loss: {train_loss} Val loss: {val_loss} Train acc: {train_acc} Val acc: {val_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
