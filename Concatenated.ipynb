{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Set GPU flag to false if running on CPU\n",
    "GPU = True\n",
    "batch_size = 32\n",
    "column1 = 'project_essay_1'\n",
    "column2 = 'project_essay_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext import data\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "TEXT = data.Field(include_lengths=True)\n",
    "ID = data.Field(sequential=False)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "\n",
    "def sort_key(ex):\n",
    "    return len(getattr(ex,column1))\n",
    "\n",
    "train, val, test = data.TabularDataset.splits(\n",
    "        path='.', train='train.csv',\n",
    "        validation='val.csv', test='dev.csv', format='csv', skip_header=True,\n",
    "        fields=[('id', ID), ('project_title', None),('project_resource_summary', None), \n",
    "                ('project_essay_1', TEXT), ('project_essay_2', TEXT), ('project_is_approved', LABEL)])\n",
    "\n",
    "#vocab is shared across all the text fields\n",
    "#CAUTION: GloVe will download all embeddings locally (862 MB).  If not interested, remove \"vectors\"\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=300))\n",
    "ID.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "#change device to 0 for GPU\n",
    "train_iter, val_iter, test_iter = data.Iterator.splits(\n",
    "        (train, val, test), repeat=False, sort=False,#sort_key= sort_key, sort_within_batch=True,\n",
    "        batch_size=(batch_size), device=-1 if GPU else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "    \n",
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab, embedding_dim, hidden_dim, vocab_size, label_size, batch_size):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.word_embeddings.weight = nn.Parameter(vocab.vectors)        \n",
    "        self.lstm = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(2*hidden_dim,2)\n",
    "\n",
    "    def init_hidden(self,batch_size):\n",
    "        if(GPU):\n",
    "            h0 = Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda())\n",
    "#             c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim).cuda())\n",
    "        \n",
    "        else:\n",
    "            h0 = Variable(torch.zeros(2, batch_size, self.hidden_dim))\n",
    "#             c0 = Variable(torch.zeros(2, batch_size, self.hidden_dim))\n",
    "            \n",
    "        return h0\n",
    "#         return (h0, c0)\n",
    "\n",
    "    def forward(self, text1,text2,):\n",
    "        \n",
    "        embeds1 = self.word_embeddings(text1)\n",
    "        embeds2 = self.word_embeddings(text2)\n",
    "        \n",
    "        self.hidden = self.init_hidden(text1.shape[1])\n",
    "        lstm_out, self.hidden = self.lstm(embeds1, self.hidden)\n",
    "        feat1 = self.hidden[0].squeeze(0)\n",
    "        \n",
    "        self.hidden = self.init_hidden(text1.shape[1])\n",
    "        lstm_out, self.hidden = self.lstm(embeds2, self.hidden)\n",
    "        feat2 = self.hidden[0].squeeze(0)\n",
    "\n",
    "        feat = torch.cat([feat1,feat2],dim=1)\n",
    "        y = self.fc(feat)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def validate(net,iter_obj):\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    iter_obj.init_epoch()\n",
    "    pred = []\n",
    "    actual = []\n",
    "    dev_loss = []\n",
    "    with torch.no_grad():\n",
    "        for batch in iter_obj:\n",
    "            input1,input2,label = getattr(batch,column1)[0], getattr(batch,column2)[0], batch.project_is_approved-1\n",
    "            if(GPU): input1, label, input2 = input1.cuda(), label.cuda(), input2.cuda()\n",
    "            scores = net(input1,input2)\n",
    "            dev_loss.append(loss_function(scores,label).item())\n",
    "            pred.extend(scores.cpu().data.numpy().argmax(axis=1))\n",
    "            actual.extend(label.data.cpu().numpy().tolist())\n",
    "    return  round(np.mean(dev_loss),4), round(accuracy_score(actual,pred),4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(NUM_EPOCHS):\n",
    "    model = LSTMClassifier(vocab = TEXT.vocab, embedding_dim=300, vocab_size=len(TEXT.vocab), \n",
    "                           hidden_dim=50, batch_size=batch_size, label_size=2)\n",
    "    \n",
    "    if(GPU): model.cuda()\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, amsgrad=True)\n",
    "\n",
    "    n_correct = 0.\n",
    "    n_total = 0.\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        train_loss = []\n",
    "        train_iter.init_epoch()\n",
    "        for batch in train_iter:\n",
    "            model.train()\n",
    "            optimizer.zero_grad()       \n",
    "            input1,input2,label = getattr(batch,column1)[0], getattr(batch,column2)[0], batch.project_is_approved-1\n",
    "            if(GPU): input1, label, input2 = input1.cuda(), label.cuda(), input2.cuda()\n",
    "            \n",
    "            scores = net(input1,input2)\n",
    "            loss = loss_function(scores, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            n_correct += (torch.max(scores, 1)[1].view(label.size()) == label).sum().item()\n",
    "            n_total += batch.batch_size\n",
    "            train_acc = 100. * n_correct/n_total\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        if(NUM_EPOCHS<=10) or ((i+1)%10==0):\n",
    "            val_loss, val_acc = validate(model,val_iter)\n",
    "            print(f\"Epoch {i+1} Train loss: {round(np.mean(train_loss),4)} Val loss: {val_loss} Train acc: {train_acc} Val acc: {val_acc}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "/home/suraj/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Train loss: 0.6934 Val loss: 0.6939 Train acc: 50.42 Val acc: 0.5\n",
      "Epoch 2 Train loss: 0.6907 Val loss: 0.6947 Train acc: 50.625 Val acc: 0.4997\n",
      "Epoch 3 Train loss: 0.6775 Val loss: 0.7133 Train acc: 51.233333333333334 Val acc: 0.5003\n",
      "Epoch 4 Train loss: 0.6474 Val loss: 0.7267 Train acc: 52.62083333333333 Val acc: 0.5283\n",
      "Epoch 5 Train loss: 0.5014 Val loss: 0.7485 Train acc: 57.06133333333333 Val acc: 0.6177\n",
      "Epoch 6 Train loss: 0.2102 Val loss: 1.0785 Train acc: 62.81944444444444 Val acc: 0.5855\n",
      "Epoch 7 Train loss: 0.0475 Val loss: 1.6903 Train acc: 67.91571428571429 Val acc: 0.5803\n",
      "Epoch 8 Train loss: 0.0121 Val loss: 2.2447 Train acc: 71.88625 Val acc: 0.5768\n",
      "Epoch 9 Train loss: 0.006 Val loss: 2.3537 Train acc: 74.99444444444444 Val acc: 0.5799\n",
      "Epoch 10 Train loss: 0.0035 Val loss: 2.381 Train acc: 77.48433333333334 Val acc: 0.5821\n",
      "CPU times: user 4min 51s, sys: 1min 12s, total: 6min 3s\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Combining essay 1 and essay 2\n",
    "model = train_net(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suraj/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:322: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train)\n",
      "/home/suraj/anaconda3/lib/python3.6/site-packages/torchtext/data/field.py:321: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  return Variable(arr, volatile=not train), lengths\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Train loss: 0.0026 Val loss: 2.1051 Train acc: 83.30666666666667 Val acc: 0.6128\n",
      "Epoch 20 Train loss: 0.0007 Val loss: 2.8417 Train acc: 91.635 Val acc: 0.614\n",
      "Epoch 30 Train loss: 0.0005 Val loss: 3.1582 Train acc: 94.41266666666667 Val acc: 0.6136\n",
      "Epoch 40 Train loss: 0.0004 Val loss: 3.0658 Train acc: 95.80291666666666 Val acc: 0.6098\n",
      "Epoch 50 Train loss: 0.0003 Val loss: 3.2577 Train acc: 96.63713333333334 Val acc: 0.6107\n",
      "Epoch 60 Train loss: 0.0003 Val loss: 3.2977 Train acc: 97.19338888888889 Val acc: 0.6109\n",
      "Epoch 70 Train loss: 0.0003 Val loss: 3.3285 Train acc: 97.5907619047619 Val acc: 0.611\n",
      "Epoch 80 Train loss: 0.0003 Val loss: 3.4714 Train acc: 97.88904166666667 Val acc: 0.6121\n",
      "Epoch 90 Train loss: 0.0003 Val loss: 3.42 Train acc: 98.1207037037037 Val acc: 0.6121\n",
      "Epoch 100 Train loss: 0.0003 Val loss: 3.4455 Train acc: 98.3062 Val acc: 0.6105\n",
      "CPU times: user 48min 52s, sys: 12min 47s, total: 1h 1min 40s\n",
      "Wall time: 1h 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Combining essay 1 and essay 2\n",
    "model = train_net(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
